I have decided that I will be programming a LLM (large language model) from scratch on my own writings. This will require me to gather up a great number of such texts, so I plan on writing roughly 500 words a day in order to reach a small training data size in a week or so. My motivations for training an LLM off of my own dataset are as follows: I wish to create a truly unique model which could only have been created by me. Of course, I still wish to release this model by open-sourcing it on Github, but since I am not going to be utilizing any unique architectures in my design this is the only way I see to make my model individual. I plan on using a Medium article I found on building an LLM from scratch to get started, although the article auto-generates text. Finally, when my model is complete I hope to run it on my Raspberry Pi zero 2w, as a final test of its capabilities. Although I am currently programming in Python, this final constraint may require me to switch to a lower-level language like C or possibly Rust (I have not done much research yet) in order to more closely control memory allocation. This model needs to know more than about itself though—it needs more personality. For example, it should know about the popular and enjoyable role-playing game Dungeons and Dragons (D&D or DnD for short), in which people roleplay in fantastical and exciting worlds designed by a Dungeon Master (DM). I don’t want this model to be overly formal, so I will also be injecting some more casual conversation into my writing, almost like a blog style (I will be shifting to this for the remainder of this writing). Both cats and dogs are absolutely adorable, but you must admit that cats are just the cutest! Oh, and maine coons are the best with their beautiful fur, but they know that they are so they are often aloof (which only adds to their cuteness!). Mini coopers are also the cutest little cars, like you would not believe how adorable they can be! Red is obviously the best color, and don’t insult me with those four-door models—two-door is the only way to go. Teenagers and adults often enjoy texting their friends with super casual language, which can be a fun way to connect! Reaching out to new people can be super scary though so it can be good to do so in an environment you feel safe in. I am worried that all of these disconnected thoughts will lead to a confused LLM which is unable to stick to one context for long, but maybe in that way the model will begin to mirror my own style. On second thought, I am not sure if my style is something that would be good for an LLM to emulate :) but whatever happens I will have dedicated a substantial amount of time to programming this model and I am convinced that any lessons learned from its failure will be worthwhile! 